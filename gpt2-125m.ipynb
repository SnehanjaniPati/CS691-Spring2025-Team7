{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11439336,"sourceType":"datasetVersion","datasetId":7165760}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Setup Data Path & Imports\nHere we point to our CSV, then import all the libraries we’ll need:\n- `datasets` for easy HuggingFace Dataset handling  \n- `transformers` for model/tokenizer/training APIs  \n- `sklearn` metrics for evaluation  \n- standard tools (NumPy, pandas, pathlib)","metadata":{}},{"cell_type":"code","source":"!pip install -q --upgrade transformers datasets evaluate scikit-learn pandas torch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data path\ndata_path = '/kaggle/input/sentim/processed_sentiment_data.csv'\n\n# Imports\nimport inspect\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom datasets import Dataset, ClassLabel\nfrom transformers import (\n    GPT2TokenizerFast, GPT2ForSequenceClassification,\n    Trainer, TrainingArguments, DataCollatorWithPadding,\n    set_seed, pipeline,\n)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T03:53:34.078367Z","iopub.execute_input":"2025-04-17T03:53:34.078630Z","iopub.status.idle":"2025-04-17T03:54:02.317786Z","shell.execute_reply.started":"2025-04-17T03:53:34.078607Z","shell.execute_reply":"2025-04-17T03:54:02.317176Z"}},"outputs":[{"name":"stderr","text":"2025-04-17 03:53:49.195444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744862029.401296      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744862029.460879      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 2. Hyper‑parameters & Random Seed\nSpecify model name, output directory, training epochs, batch size, learning rate,\nrandom seed for reproducibility, and maximum sequence length.","metadata":{}},{"cell_type":"code","source":"model_name   = \"gpt2\"\noutput_dir   = \"./sentiment-gpt2\"\nepochs       = 4\nbatch_size   = 8\nlr           = 2e-5\nseed         = 42\nmax_len      = 128\n\n# set global seed\nset_seed(seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T03:54:02.318897Z","iopub.execute_input":"2025-04-17T03:54:02.319542Z","iopub.status.idle":"2025-04-17T03:54:02.328504Z","shell.execute_reply.started":"2025-04-17T03:54:02.319518Z","shell.execute_reply":"2025-04-17T03:54:02.327719Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# 3. Load & Prepare Dataset\n- Read CSV and ensure it has `text` & `sentiment` columns  \n- Map sentiment strings → integer labels  \n- Wrap into a HuggingFace `Dataset` and split into train/test  \n- Cast labels to `ClassLabel` for proper handling\n","metadata":{}},{"cell_type":"code","source":"def load_dataset(csv_path: Path):\n    df = pd.read_csv(csv_path)\n    if {\"text\", \"sentiment\"} - set(df.columns):\n        raise ValueError(\"CSV needs ‘text’ and ‘sentiment’ cols\")\n    df[\"text\"] = df[\"text\"].fillna(\"\")\n    labels   = sorted(df[\"sentiment\"].unique())\n    label2id = {lbl: i for i, lbl in enumerate(labels)}\n    id2label = {i: lbl for lbl, i in label2id.items()}\n    df[\"label\"] = df[\"sentiment\"].map(label2id)\n    return Dataset.from_pandas(df[[\"text\", \"label\"]]), label2id, id2label\n\n# load\ndataset, label2id, id2label = load_dataset(Path(data_path))\n\n# create ClassLabel and cast\nclass_label = ClassLabel(num_classes=len(label2id), names=list(label2id.keys()))\ndataset     = dataset.cast_column(\"label\", class_label)\n\n# train/test split\ntrain_ds, test_ds = dataset.train_test_split(\n    test_size=0.2, stratify_by_column=\"label\"\n).values()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T03:54:02.329408Z","iopub.execute_input":"2025-04-17T03:54:02.329675Z","iopub.status.idle":"2025-04-17T03:54:02.459224Z","shell.execute_reply.started":"2025-04-17T03:54:02.329652Z","shell.execute_reply":"2025-04-17T03:54:02.458694Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/823 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab525f2248a348c5bb0d75885d7650e2"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# 4. Tokenizer & Model\n- Load GPT‐2 tokenizer, set padding token to EOS  \n- Load GPT‑2 classification head with correct `num_labels`  \n- Resize embeddings and set `pad_token_id`\n","metadata":{}},{"cell_type":"code","source":"# tokenizer\ntok = GPT2TokenizerFast.from_pretrained(model_name)\ntok.pad_token = tok.eos_token\n\n# model\nmodel = GPT2ForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(label2id),\n    id2label=id2label,\n    label2id=label2id,\n)\nmodel.resize_token_embeddings(len(tok))\nmodel.config.pad_token_id = tok.pad_token_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T03:54:02.460598Z","iopub.execute_input":"2025-04-17T03:54:02.460825Z","iopub.status.idle":"2025-04-17T03:54:09.854734Z","shell.execute_reply.started":"2025-04-17T03:54:02.460808Z","shell.execute_reply":"2025-04-17T03:54:09.854054Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"611af047ff7643cda87ab4c44f463dc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb13d4f9d514d20b84e75ba33c7cdd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9ddd46e1d3e4aff9fd05ea98c28e35a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33deaacba9bb45deba01a83b623895c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c427ca219174dc9bed4fcc31280f75d"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7905001adba34e039494f542fc4d6332"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# 5. Tokenization Helper\nDefine a function to tokenize the `text` field, truncating/padding to `max_len`.\n","metadata":{}},{"cell_type":"code","source":"def tokenize(ds):\n    return ds.map(\n        lambda x: tok(\n            x[\"text\"], \n            truncation=True, \n            padding=\"max_length\", \n            max_length=max_len\n        ),\n        batched=True,\n        remove_columns=[\"text\"]\n    )\n\ntrain_tok = tokenize(train_ds)\ntest_tok  = tokenize(test_ds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T03:54:09.855620Z","iopub.execute_input":"2025-04-17T03:54:09.855997Z","iopub.status.idle":"2025-04-17T03:54:10.436147Z","shell.execute_reply.started":"2025-04-17T03:54:09.855965Z","shell.execute_reply":"2025-04-17T03:54:10.435327Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/658 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"628f5db355c64f48829c1b8b8893f40a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/165 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd52664d37ce49348599e6d85e2cebbf"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# 6. Trainer Setup\n- Build `TrainingArguments`\n- Define simple accuracy metric  \n- Instantiate `Trainer`\n","metadata":{}},{"cell_type":"code","source":"# base args\nta_kwargs = dict(\n    output_dir=output_dir,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    report_to=\"none\",\n    seed=seed,\n)\n\n# check for eval arg name\nsig = inspect.signature(TrainingArguments.__init__)\nif \"evaluation_strategy\" in sig.parameters:\n    ta_kwargs[\"evaluation_strategy\"] = \"epoch\"\nelse:\n    ta_kwargs[\"eval_strategy\"] = \"epoch\"\n\ntraining_args = TrainingArguments(**ta_kwargs)\n\n# metrics\ndef metrics(pred):\n    logits, labels = pred\n    return {\"accuracy\": accuracy_score(labels, np.argmax(logits, -1))}\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tok,\n    eval_dataset=test_tok,\n    tokenizer=tok,\n    data_collator=DataCollatorWithPadding(tok),\n    compute_metrics=metrics,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T03:54:10.437119Z","iopub.execute_input":"2025-04-17T03:54:10.437423Z","iopub.status.idle":"2025-04-17T03:54:10.862038Z","shell.execute_reply.started":"2025-04-17T03:54:10.437404Z","shell.execute_reply":"2025-04-17T03:54:10.861139Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3033315351.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# 7. Train & Evaluate\nFit the model, then print overall eval metrics and a detailed classification report.\n","metadata":{}},{"cell_type":"code","source":"# train\ntrainer.train()\n\n# evaluation\nprint(\"\\n*** Evaluation Metrics ***\\n\", trainer.evaluate())\n\n# detailed report\npreds = np.argmax(trainer.predict(test_tok).predictions, -1)\nprint(\"\\nClassification Report:\\n\",\n      classification_report(test_tok[\"label\"], preds,\n                            target_names=class_label.names))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T03:54:10.862755Z","iopub.execute_input":"2025-04-17T03:54:10.863464Z","iopub.status.idle":"2025-04-17T03:55:29.251834Z","shell.execute_reply.started":"2025-04-17T03:54:10.863436Z","shell.execute_reply":"2025-04-17T03:55:29.251088Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [168/168 01:13, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.965700</td>\n      <td>0.502568</td>\n      <td>0.878788</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.532000</td>\n      <td>0.489597</td>\n      <td>0.878788</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.449300</td>\n      <td>0.501029</td>\n      <td>0.884848</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.409100</td>\n      <td>0.523528</td>\n      <td>0.878788</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n*** Evaluation Metrics ***\n {'eval_loss': 0.5010291934013367, 'eval_accuracy': 0.8848484848484849, 'eval_runtime': 1.3151, 'eval_samples_per_second': 125.468, 'eval_steps_per_second': 8.365, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nClassification Report:\n               precision    recall  f1-score   support\n\n     LABEL_0       0.88      1.00      0.94       139\n     LABEL_1       1.00      0.47      0.64        15\n     LABEL_2       0.00      0.00      0.00        11\n\n    accuracy                           0.88       165\n   macro avg       0.63      0.49      0.52       165\nweighted avg       0.83      0.88      0.85       165\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 8. Save model & Demo\n- Save model + tokenizer to `output_dir`  \n- Run a quick pipeline demo on two sample sentences\n","metadata":{}},{"cell_type":"code","source":"# save\ntrainer.save_model(output_dir)\ntok.save_pretrained(output_dir)\nprint(f\"\\nModel & tokenizer saved to {output_dir}\")\n\n# inference demo\nclf = pipeline(\"sentiment-analysis\", model=output_dir, tokenizer=output_dir, top_k=1)\nfor txt in [\n    \"I absolutely loved this product!\",\n    \"This is the worst experience I’ve ever had.\"\n]:\n    print(f\"» {txt!r} → {clf(txt)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T03:55:36.222008Z","iopub.execute_input":"2025-04-17T03:55:36.222318Z","iopub.status.idle":"2025-04-17T03:55:38.180929Z","shell.execute_reply.started":"2025-04-17T03:55:36.222287Z","shell.execute_reply":"2025-04-17T03:55:38.180316Z"}},"outputs":[{"name":"stdout","text":"\nModel & tokenizer saved to ./sentiment-gpt2\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"» 'I absolutely loved this product!' → [[{'label': 'LABEL_0', 'score': 0.9773532152175903}]]\n» 'This is the worst experience I’ve ever had.' → [[{'label': 'LABEL_0', 'score': 0.9559566974639893}]]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}